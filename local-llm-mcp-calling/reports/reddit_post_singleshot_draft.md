# Reddit Post Draft
# Target: r/LocalLLaMA (primary), r/selfhosted, r/OpenSourceAI
# Tone: technical, honest, community-first

---

**TITLE OPTIONS** (pick one):

1. `I benchmarked [N] local models on real MCP tool calling — here are the results`
2. `Which local LLMs can actually use MCP tools? I tested [N] models on a real project management API`
3. `MCP tool calling benchmark: [N] local models, 3 difficulty levels, real API — full results`

---

## POST BODY

---

I've been building [Workunit](https://workunit.app), a project manager designed for AI agents — the kind of tool where you tell an AI "close out this sprint" and it actually does it: marks tasks done, saves context notes, completes the workunit. All through MCP.

When I started showing it to people, the first question was always: *"Does it work with local models?"*

I didn't know. So I found out.

---

### What I tested

I set up a benchmark with **[N] models** running locally via LM Studio, all interacting with Workunit's MCP server through OpenCode. Each model was tested on **28 tasks across 3 difficulty levels**:

**Level 0 — Explicit** *(automated, Python → LM Studio API)*
Tell the model exactly what to call and with what parameters. This tests whether it can emit a valid tool call at all.

> *"Call `create_workunit` with name='Hello World', problem_statement='...', success_criteria='...', priority='normal'"*

**Level 1 — Natural Language** *(via OpenCode)*
Human-style request with the key information present but not structured. The model must identify the right tool and map the description to parameter names.

> *"Create a workunit called 'Fix Login Page Bug'. The problem is users can't log in with special characters in their password. We'll know it's done when all character types work and there are regression tests. High priority."*

**Level 2 — Reasoning** *(via OpenCode)*
High-level goal. No tool names. No parameter hints. The model has to figure out the sequence, chain IDs across calls, and make decisions about structure.

> *"End of sprint. Mark all todo tasks done, save a summary of what was accomplished, and complete the workunit."*

---

### The tools being tested

Workunit's MCP server exposes 21 tools: create/get/update for projects, workunits, tasks, assets; search; save_context (structured decision/insight/progress atoms); directory management; etc.

The hard parts:
- `update_mask: { paths: ["status", "completion_notes"] }` — a nested object most models get wrong
- Chaining: use the `id` from `create_project` as the `project_id` in `create_workunit`
- Enum values: `"in_progress"` not `"in progress"` or `"IN_PROGRESS"`
- Conditional fields: `completion_notes` only makes sense when `status="completed"`

---

### Results

*(results table generated by aggregate_results.py after runs complete)*

| Model | L0 Pass% | L1 Pass% | L2 Pass% | Overall |
|-------|----------|----------|----------|---------|
| [TBD after runs] | | | | |

---

### Observations

*(fill in after runs)*

---

### Reproduce it yourself

Everything is open source and fully documented:

**Requirements:**
- LM Studio (free, runs the models)
- OpenCode (open source, the AI coding agent used for L1/L2)
- Workunit dev environment or a free account at workunit.app
- Python 3.10+ (`pip install openai rich`)

**Run Level 0 (automated):**
```bash
git clone https://github.com/[your-repo]
cd workunit/benchmark
python scripts/run_level0.py --model <your-model-id>
```

**Run Level 1 & 2:**
Follow the guide in `benchmark/scripts/opencode_config.md` — it's a manual process through OpenCode, takes about 30-45 minutes per model.

**The task definitions** are plain JSON in `benchmark/tasks/` — you can read exactly what each prompt is and what the validation criteria are.

---

### What I couldn't test

My 4080 16GB tops out around 32-34B at 4-bit. I'd love results for:
- Llama 3.3 70B (needs 40GB+)
- Qwen2.5-Coder 72B
- DeepSeek-R1 70B
- Mistral Large
- Mixtral 8x22B

If you have the hardware, the benchmark is ready to run. Drop your results in the comments or as a PR to the repo.

---

### About Workunit

It's a project manager built around structured AI context — each workunit has a problem statement, tasks, and a trail-of-thought that the AI writes back as it works. The idea is you define the work once, then any AI (Claude, GPT, Gemini, local model) picks it up through MCP with full context. Decisions, failed attempts, architectural choices — all preserved.

Free tier available. I built it because I was tired of re-explaining my codebase every session.

Happy to answer questions about the benchmark methodology or the platform.

— Alyx

---

## SUBREDDITS TO POST IN

**Primary (post here first, within a few days of each other):**
1. **r/LocalLLaMA** — Main hub, 500k+ members, most relevant audience. Post first.
2. **r/selfhosted** — Strong overlap with local LLM crowd, loves technical comparisons
3. **r/SideProject** — You already have the post ready (your existing draft)

**Secondary (post after primary ones have settled):**
4. **r/OpenSourceAI** — Open source credibility angle
5. **r/LMStudio** — Direct users of your exact toolchain
6. **r/AIAssistants** — Tool use angle fits perfectly
7. **r/ClaudeAI** — Interesting crossover: Claude users curious about local alternatives

**Maybe (larger/noisier, lower signal-to-noise):**
8. **r/Artificial** — General AI, wider reach but less technical
9. **r/ChatGPT** — Very large but off-topic for local models, skip probably

**Posting strategy:**
- r/LocalLLaMA first — it will generate the most technical discussion and help you catch any methodology issues before wider distribution
- Wait 2-3 days between posts, tailor the title slightly for each community
- In r/selfhosted: lead with "self-hosted AI agents" angle, not just benchmark
- In r/LMStudio: lead with "I used LM Studio to run this" specifically

---

## NOTES FOR FINAL DRAFT

- Fill in actual model list once downloads complete
- Fill in actual results after runs
- Add a screenshot of the results table (visual beats a wall of text on Reddit)
- Consider a short video showing OpenCode making an MCP call — very compelling
- The "I couldn't test 70B+" section is important: it explicitly invites community participation
